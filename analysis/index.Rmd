---
title: "Home"
site: workflowr::wflow_site
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE,
                      autodep = TRUE,
                      cache.lazy = FALSE,
                      message = FALSE, 
                      dpi = 180,
                      fig.width = 8, 
                      fig.height = 5)

pacman::p_load(tidyverse,
               janitor,
               here,
               readxl,
               kableExtra)

ggplot2::theme_set(theme_minimal())

```


# Introduction

This notebook will host the practice lab for the classic book [Introudction to Statistical Learning](https://www.statlearning.com/)
but now with an application of the [tidymodels](https://www.tidymodels.org/) principles.

This was created by [Emil Hvitfeldt](https://www.emilhvitfeldt.com/)

The purpose is to learn, practice and familiarize myself with the `tidymodels` suite
using case studies.

The author of the blog went through all the chapters of the book, [which is also 
available online](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf),  using the 
tidymodels packages in R.  

Also the book comes with an R package itself that hosts many of the data sets used 
in the examples:

![](https://i.imgur.com/iGYmZ9c.png)

The original book was writting not taking into account the tidyverse style guide.
Since tidymodels is built on the same premises.  The labs will not ressemble the ones 
in the book, and that is all good enough for me.

One disclaimer is that I haven't studied the tidymodels suite in details (or at all,) 
so I am attempting to learn by doing the excercises in these labs.  I understand I 
may incur in a few interpretation errors due to skipping the prerequisite mentioned.


## Installing the packages


Several packages will be needed to run these labs.  In the website, they are uploaded 
as required.  I will update this section to add other packages as they appear. 

The tidymodels suite contains the folloinng: `parsnip`, `recipes and workflows`, `rsample`, and `tune`

```{r installing the packages}
# Install tidymodels and ISLR
pacman::p_load(tidymodels, # for parsnip, recipes, etc...
               tidyverse,  # for dplyr, ggplot, purrr, etc...
               ISLR,       # for the data sets indicated in the Introduction
               MASS)       # for the Boston data set

```


# Linear Regression

The first lab in ISLR is about doing the first method for predictions which is getting 
a linear regression model.  

Linear regression is a very simple model based on Supervised learning, that is, working 
with labeled data for classification or prediction outcomes.  Linear regression is often 
used to learn from the data to make predictions.  

In this lab we will work with a Simple regression and a multiple regression model.

## Simple linear regression

For this example we will use the `Boston` data set that has information fo 506 neighborhodds 
in Boston.  This data set is very outdated and some variables are very unfortunate according 
to the author of the blog.

Let's check the data set.

```{r checking the Boston data set}
MASS::Boston %>% 
  dplyr::glimpse()

```


Simple linear models are used to compare how one variable is affected by another.  
In this case, we will build a model that relates the median value (`medz`)of homes as the 
response variable (y), with the percentage of population that belongs to lower status (`lstat`)
as the predictor (x.)

## Building the model

First we need to create a specification that the model we will use is a linear regression model.  
The `parsnip` package is used for that.  

```{r create a parsnip specification model}
lm_spec <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

```

The previous code does not do any calculation.  It is just a specification of the model 
we want to create, what we want to do.

```{r checking the specification}
lm_spec

```

[Back to Top](#introduction)

After having the model specified, we can now fit it by supplying the formulat and 
the data.  The formula is written on the form `y ~ x`, and the data we'll use will 
be the one from the Boston data set.  We are not dividing the data into training and
testing just yet.  

The names of the variable should match the names of the variabls (columns) in 
the data set.

```{r fitting the data to the specification}
lm_fit <- lm_spec %>% 
  fit(medv ~ lstat, data = Boston)

lm_fit

```

The previous result is a parsnip object

```{r checking the lm_fit object}
lm_fit %>% 
  class()

```

To see the underlying fit object, we can check the fit list element.

```{r checking the lm_Fit fit object using base R}
lm_fit$fit

```

or by using `purrr::pluck()`

```{r checking the lm_fit object using pluck}
lm_fit %>% 
  pluck("fit")

```

We can check the summary but adding that function into the previous code

```{r checking the summary from the lm object}
lm_fit %>% 
  pluck("fit") %>% 
  summary()

```


From the `Broom` package we can get the summary in a tidy manner.

```{r checking the summary from the lm object using broom}
lm_fit %>% 
  broom::tidy()

```

And `glance()` is used to extract the models statistics

```{r using glance() to extract model stats}
lm_fit %>% 
  broom::glance() %>% 
  pivot_longer(everything(),
               names_to = "stats", 
               values_to = "values") %>% 
  mutate(across(values, round, 5))

```


## Getting predictions on the simple regression model

Supposing we like the fit model, we can then generate predictions based on it.  

Important to pass the data set where we based the predictions on.

```{r}
lm_fit %>% 
  predict(new_data = Boston)
  
```

To compare the observed values with the predicted ones we can use `augment()`

```{r}
# augment(lm_fit, new_data = Boston) %>% 
# select(medv, .pred)

```


[Back to Top](#introduction)




















































